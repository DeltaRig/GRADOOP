<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" lang="en"><head><meta http-equiv="Content-Type" content="text/html;charset=UTF-8"/><link rel="stylesheet" href="../jacoco-resources/report.css" type="text/css"/><link rel="shortcut icon" href="../jacoco-resources/report.gif" type="image/gif"/><title>DualSimulation.java</title><link rel="stylesheet" href="../jacoco-resources/prettify.css" type="text/css"/><script type="text/javascript" src="../jacoco-resources/prettify.js"></script></head><body onload="window['PR_TAB_WIDTH']=4;prettyPrint()"><div class="breadcrumb" id="breadcrumb"><span class="info"><a href="../jacoco-sessions.html" class="el_session">Sessions</a></span><a href="../index.html" class="el_report">Gradoop Flink</a> &gt; <a href="index.source.html" class="el_package">org.gradoop.flink.model.impl.operators.matching.single.simulation.dual</a> &gt; <span class="el_source">DualSimulation.java</span></div><h1>DualSimulation.java</h1><pre class="source lang-java linenums">/*
 * Copyright Â© 2014 - 2021 Leipzig University (Database Research Group)
 *
 * Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.gradoop.flink.model.impl.operators.matching.single.simulation.dual;

import org.apache.flink.api.java.DataSet;
import org.apache.flink.api.java.operators.DeltaIteration;
import org.apache.flink.api.java.operators.IterativeDataSet;
import org.apache.flink.api.java.tuple.Tuple1;
import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;
import org.gradoop.common.model.api.entities.Edge;
import org.gradoop.common.model.api.entities.GraphHead;
import org.gradoop.common.model.api.entities.Vertex;
import org.gradoop.common.model.impl.id.GradoopId;
import org.gradoop.flink.model.api.epgm.BaseGraph;
import org.gradoop.flink.model.api.epgm.BaseGraphCollection;
import org.gradoop.flink.model.api.epgm.BaseGraphCollectionFactory;
import org.gradoop.flink.model.api.epgm.BaseGraphFactory;
import org.gradoop.flink.model.impl.functions.epgm.Id;
import org.gradoop.flink.model.impl.functions.epgm.VertexFromId;
import org.gradoop.flink.model.impl.functions.utils.RightSide;
import org.gradoop.flink.model.impl.operators.matching.common.PostProcessor;
import org.gradoop.flink.model.impl.operators.matching.common.PreProcessor;
import org.gradoop.flink.model.impl.operators.matching.common.tuples.TripleWithCandidates;
import org.gradoop.flink.model.impl.operators.matching.single.PatternMatching;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.debug.PrintDeletion;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.debug.PrintFatVertex;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.debug.PrintMessage;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.functions.BuildFatVertex;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.functions.CloneAndReverse;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.functions.CombinedMessages;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.functions.GroupedFatVertices;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.functions.GroupedMessages;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.functions.UpdateVertexState;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.functions.UpdatedFatVertices;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.functions.ValidFatVertices;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.functions.ValidateNeighborhood;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.tuples.Deletion;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.tuples.FatVertex;
import org.gradoop.flink.model.impl.operators.matching.single.simulation.dual.tuples.Message;

import static org.gradoop.flink.model.impl.operators.matching.common.debug.Printer.log;

/**
 * Vertex-centric Dual-Simulation.
 *
 * @param &lt;G&gt; The graph head type.
 * @param &lt;V&gt; The vertex type.
 * @param &lt;E&gt; The edge type.
 * @param &lt;LG&gt; The graph type.
 * @param &lt;GC&gt; The graph collection type.
 */
public class DualSimulation&lt;
  G extends GraphHead,
  V extends Vertex,
  E extends Edge,
  LG extends BaseGraph&lt;G, V, E, LG, GC&gt;,
  GC extends BaseGraphCollection&lt;G, V, E, LG, GC&gt;&gt; extends PatternMatching&lt;G, V, E, LG, GC&gt; {

  /**
   * Logger
   */
<span class="fc" id="L76">  private static Logger LOG = LogManager.getLogger(DualSimulation.class);</span>

  /**
   * If true, the algorithm uses bulk iteration for the core iteration.
   * Otherwise it uses delta iteration.
   */
  private final boolean useBulkIteration;

  /**
   * Creates a new operator instance.
   *
   * @param query       GDL based query
   * @param attachData  attach original data to resulting vertices/edges
   * @param useBulk     true to use bulk, false to use delta iteration
   */
  public DualSimulation(String query, boolean attachData, boolean useBulk) {
<span class="fc" id="L92">    super(query, attachData, LOG);</span>
<span class="fc" id="L93">    this.useBulkIteration = useBulk;</span>
<span class="fc" id="L94">  }</span>

  @Override
  protected GC executeForVertex(LG graph)  {
<span class="fc" id="L98">    DataSet&lt;Tuple1&lt;GradoopId&gt;&gt; matchingVertexIds = PreProcessor</span>
<span class="fc" id="L99">      .filterVertices(graph, getQuery())</span>
<span class="fc" id="L100">      .project(0);</span>

<span class="fc" id="L102">    BaseGraphFactory&lt;G, V, E, LG, GC&gt; graphFactory = graph.getFactory();</span>
<span class="fc" id="L103">    BaseGraphCollectionFactory&lt;G, V, E, LG, GC&gt; collectionFactory = graph.getCollectionFactory();</span>

<span class="fc bfc" id="L105" title="All 2 branches covered.">    if (doAttachData()) {</span>
<span class="fc" id="L106">      return collectionFactory.fromGraph(</span>
<span class="fc" id="L107">        graphFactory.fromDataSets(matchingVertexIds</span>
<span class="fc" id="L108">            .join(graph.getVertices())</span>
<span class="fc" id="L109">            .where(0).equalTo(new Id&lt;&gt;())</span>
<span class="fc" id="L110">            .with(new RightSide&lt;&gt;())));</span>
    } else {
<span class="fc" id="L112">      return collectionFactory.fromGraph(</span>
<span class="fc" id="L113">        graphFactory.fromDataSets(matchingVertexIds</span>
<span class="fc" id="L114">            .map(new VertexFromId&lt;&gt;(graphFactory.getVertexFactory()))));</span>
    }
  }

  /**
   * Performs dual simulation based on the given query.
   *
   * @param graph data graph
   * @return match graph
   */
  protected GC executeForPattern(LG graph) {
    //--------------------------------------------------------------------------
    // Pre-processing (filter candidates + build initial working set)
    //--------------------------------------------------------------------------

<span class="fc" id="L129">    DataSet&lt;TripleWithCandidates&lt;GradoopId&gt;&gt; triples = filterTriples(graph);</span>
<span class="fc" id="L130">    DataSet&lt;FatVertex&gt; fatVertices = buildInitialWorkingSet(triples);</span>

    //--------------------------------------------------------------------------
    // Dual Simulation
    //--------------------------------------------------------------------------

<span class="fc bfc" id="L136" title="All 2 branches covered.">    DataSet&lt;FatVertex&gt; result = useBulkIteration ?</span>
<span class="fc" id="L137">      simulateBulk(fatVertices) : simulateDelta(fatVertices);</span>

    //--------------------------------------------------------------------------
    // Post-processing (build maximum match graph)
    //--------------------------------------------------------------------------

<span class="fc" id="L143">    return postProcess(graph, result);</span>
  }

  /**
   * Extracts valid triples from the input graph based on the query.
   *
   * @param g input graph
   * @return triples that have a match in the query graph
   */
  private DataSet&lt;TripleWithCandidates&lt;GradoopId&gt;&gt; filterTriples(LG g) {
    // filter vertex-edge-vertex triples by query predicates
<span class="fc" id="L154">    return PreProcessor.filterTriplets(g, getQuery());</span>
  }

  /**
   * Prepares the initial working set for the bulk iteration.
   *
   * @param triples matching triples from the input graph
   * @return data set containing fat vertices
   */
  private DataSet&lt;FatVertex&gt; buildInitialWorkingSet(DataSet&lt;TripleWithCandidates&lt;GradoopId&gt;&gt; triples) {
<span class="fc" id="L164">    return triples.flatMap(new CloneAndReverse())</span>
<span class="fc" id="L165">      .groupBy(1) // sourceId</span>
<span class="fc" id="L166">      .combineGroup(new BuildFatVertex(getQuery()))</span>
<span class="fc" id="L167">      .groupBy(0) // vertexId</span>
<span class="fc" id="L168">      .reduceGroup(new GroupedFatVertices());</span>
  }

  /**
   * Performs dual simulation using bulk iteration.
   *
   * @param vertices fat vertices
   * @return remaining fat vertices after dual simulation
   */
  private DataSet&lt;FatVertex&gt; simulateBulk(DataSet&lt;FatVertex&gt; vertices) {

<span class="fc" id="L179">    vertices = log(vertices, new PrintFatVertex(false, &quot;iteration start&quot;),</span>
<span class="fc" id="L180">      getVertexMapping(), getEdgeMapping());</span>

    // ITERATION HEAD
<span class="fc" id="L183">    IterativeDataSet&lt;FatVertex&gt; workSet = vertices.iterate(Integer.MAX_VALUE);</span>

    // ITERATION BODY

    // validate neighborhood of each vertex and create deletions
<span class="fc" id="L188">    DataSet&lt;Deletion&gt; deletions = workSet</span>
<span class="fc" id="L189">      .filter(new UpdatedFatVertices())</span>
<span class="fc" id="L190">      .flatMap(new ValidateNeighborhood(getQuery()));</span>

<span class="fc" id="L192">    deletions = log(deletions, new PrintDeletion(true, &quot;deletion&quot;),</span>
<span class="fc" id="L193">      getVertexMapping(), getEdgeMapping());</span>

    // combine deletions to message
<span class="fc" id="L196">    DataSet&lt;Message&gt; combinedMessages = deletions</span>
<span class="fc" id="L197">      .groupBy(0)</span>
<span class="fc" id="L198">      .combineGroup(new CombinedMessages());</span>

<span class="fc" id="L200">    combinedMessages = log(combinedMessages, new PrintMessage(true, &quot;combined&quot;),</span>
<span class="fc" id="L201">      getVertexMapping(), getEdgeMapping());</span>

    // group messages to final message
<span class="fc" id="L204">    DataSet&lt;Message&gt; messages = combinedMessages</span>
<span class="fc" id="L205">      .groupBy(0)</span>
<span class="fc" id="L206">      .reduceGroup(new GroupedMessages());</span>

<span class="fc" id="L208">    messages = log(messages, new PrintMessage(true, &quot;grouped&quot;),</span>
<span class="fc" id="L209">      getVertexMapping(), getEdgeMapping());</span>

    // update candidates and build next working set
<span class="fc" id="L212">    DataSet&lt;FatVertex&gt; nextWorkingSet = workSet</span>
<span class="fc" id="L213">      .leftOuterJoin(messages)</span>
<span class="fc" id="L214">      .where(0).equalTo(0) // vertexId == recipientId</span>
<span class="fc" id="L215">      .with(new UpdateVertexState(getQuery()))</span>
<span class="fc" id="L216">      .filter(new ValidFatVertices());</span>

<span class="fc" id="L218">    nextWorkingSet = log(nextWorkingSet,</span>
      new PrintFatVertex(true, &quot;next workset&quot;),
<span class="fc" id="L220">      getVertexMapping(), getEdgeMapping());</span>

    // ITERATION FOOTER
<span class="fc" id="L223">    return workSet.closeWith(nextWorkingSet, deletions);</span>
  }

  /**
   * Performs dual simulation using delta iteration.
   *
   * @param vertices fat vertices
   * @return remaining fat vertices after dual simulation
   */
  private DataSet&lt;FatVertex&gt; simulateDelta(DataSet&lt;FatVertex&gt; vertices) {
    // prepare initial working set
<span class="fc" id="L234">    DataSet&lt;Message&gt; initialWorkingSet = vertices</span>
<span class="fc" id="L235">      .flatMap(new ValidateNeighborhood(getQuery()))</span>
<span class="fc" id="L236">      .groupBy(0)</span>
<span class="fc" id="L237">      .combineGroup(new CombinedMessages())</span>
<span class="fc" id="L238">      .groupBy(0)</span>
<span class="fc" id="L239">      .reduceGroup(new GroupedMessages());</span>

<span class="fc" id="L241">    vertices = log(vertices, new PrintFatVertex(false, &quot;initial solution set&quot;),</span>
<span class="fc" id="L242">      getVertexMapping(), getEdgeMapping());</span>

<span class="fc" id="L244">    initialWorkingSet = log(initialWorkingSet,</span>
      new PrintMessage(false, &quot;initial working set&quot;),
<span class="fc" id="L246">      getVertexMapping(), getEdgeMapping());</span>

    // ITERATION HEAD
<span class="fc" id="L249">    DeltaIteration&lt;FatVertex, Message&gt; iteration = vertices</span>
<span class="fc" id="L250">      .iterateDelta(initialWorkingSet, Integer.MAX_VALUE, 0);</span>

    // ITERATION BODY

    // get updated vertices
<span class="fc" id="L255">    DataSet&lt;FatVertex&gt; deltas = iteration.getSolutionSet()</span>
<span class="fc" id="L256">      .join(iteration.getWorkset())</span>
<span class="fc" id="L257">      .where(0).equalTo(0)</span>
<span class="fc" id="L258">      .with(new UpdateVertexState(getQuery()));</span>

<span class="fc" id="L260">    deltas = log(deltas, new PrintFatVertex(true, &quot;solution set delta&quot;),</span>
<span class="fc" id="L261">      getVertexMapping(), getEdgeMapping());</span>

    // prepare new messages for the next round from updates
<span class="fc" id="L264">    DataSet&lt;Message&gt; updates = deltas</span>
<span class="fc" id="L265">      .filter(new ValidFatVertices())</span>
<span class="fc" id="L266">      .flatMap(new ValidateNeighborhood(getQuery()))</span>
<span class="fc" id="L267">      .groupBy(0)</span>
<span class="fc" id="L268">      .combineGroup(new CombinedMessages())</span>
<span class="fc" id="L269">      .groupBy(0)</span>
<span class="fc" id="L270">      .reduceGroup(new GroupedMessages());</span>

<span class="fc" id="L272">    updates = log(updates, new PrintMessage(true, &quot;next working set&quot;),</span>
<span class="fc" id="L273">      getVertexMapping(), getEdgeMapping());</span>

    // ITERATION FOOTER
    // filter vertices with no candidates after iteration
<span class="fc" id="L277">    return iteration.closeWith(deltas, updates).filter(new ValidFatVertices());</span>
  }

  /**
   * Extracts vertices and edges from the query result and constructs a
   * maximum match graph.
   *
   * @param graph    input graph
   * @param vertices valid vertices after simulation
   * @return maximum match graph
   */
  private GC postProcess(LG graph, DataSet&lt;FatVertex&gt; vertices) {
<span class="fc" id="L289">    BaseGraphFactory&lt;G, V, E, LG, GC&gt; graphFactory = graph.getFactory();</span>

<span class="fc bfc" id="L291" title="All 2 branches covered.">    DataSet&lt;V&gt; matchVertices = doAttachData() ?</span>
<span class="fc" id="L292">      PostProcessor.extractVerticesWithData(vertices, graph.getVertices()) :</span>
<span class="fc" id="L293">      PostProcessor.extractVertices(vertices, graphFactory.getVertexFactory());</span>

<span class="fc bfc" id="L295" title="All 2 branches covered.">    DataSet&lt;E&gt; matchEdges = doAttachData() ?</span>
<span class="fc" id="L296">      PostProcessor.extractEdgesWithData(vertices, graph.getEdges()) :</span>
<span class="fc" id="L297">      PostProcessor.extractEdges(vertices, graphFactory.getEdgeFactory());</span>

<span class="fc" id="L299">    return graph.getCollectionFactory().fromGraph(graphFactory.fromDataSets(matchVertices, matchEdges));</span>
  }
}
</pre><div class="footer"><span class="right">Created with <a href="http://www.jacoco.org/jacoco">JaCoCo</a> 0.8.1.201803210924</span></div></body></html>